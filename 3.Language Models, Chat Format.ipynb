{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "275e254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"API_KEY\"\n",
    "# Set your Gemini API key\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "# Now you can create a model\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337454ba",
   "metadata": {},
   "source": [
    "#### helper function\n",
    "Low Temperature (e.g., 0.2): The model tends to produce more focused and deterministic output. It is more likely to choose the most probable next word based on its training data.\n",
    "\n",
    "Medium Temperature (e.g., 0.5): A balance between randomness and focus. It allows for some variability in the output, making it less predictable than low temperature but not as random as high temperature.\n",
    "\n",
    "High Temperature (e.g., 0.8 or 1.0): The output becomes more creative and diverse. The model is more likely to introduce less common words and phrases, resulting in more varied and sometimes unpredictable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3230453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, temperature=0.2):\n",
    "    response = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config={\"temperature\": temperature}\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5cd490",
   "metadata": {},
   "source": [
    "## Prompt the model and get a completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae98b08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2948714a",
   "metadata": {},
   "source": [
    "##Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bec4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letters in \"lollipop\" are:\n",
      "\n",
      "l, o, l, l, i, p, o, p\n",
      "\n",
      "Reversing them gives:\n",
      "\n",
      "p, o, p, i, l, l, o, l\n"
     ]
    }
   ],
   "source": [
    "response=get_completion('Take the letter in lollipop \\ and reverse them')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae12d8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The letters in \"lollipop\" are:\\n\\nl, o, l, l, i, p, o, p\\n\\nReversing them gives:\\n\\n**p, o, p, i, l, l, o, l**\\n\\nSo, the reversed word is **popillol**.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(\"\"\"Take the letters in \\\n",
    "l-o-l-l-i-p-o-p and reverse them\"\"\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659c8834",
   "metadata": {},
   "source": [
    "## Helper function (chat format)\n",
    "Here's the helper function we'll use in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a501edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure your API key\n",
    "genai.configure(api_key=\"API_KEY\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "\n",
    "def get_completion_from_messages(messages, temperature=0, max_tokens=500):\n",
    "  \n",
    "    gemini_messages = []\n",
    "    for msg in messages:\n",
    "        gemini_messages.append({\n",
    "            \"role\": \"user\" if msg[\"role\"] == \"user\" else \"model\",\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "    \n",
    "    # Generate response from Gemini\n",
    "    response = model.generate_content(\n",
    "        gemini_messages,\n",
    "        generation_config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_tokens\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f38c887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the happy young carrot, so bright and so grand,\n",
      "He wiggled his roots all across the green land!\n",
      "With a grin on his face, and a sparkle so keen,\n",
      "The happiest carrot you've ever yet seen!\n"
     ]
    }
   ],
   "source": [
    "messages =  [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who\\\n",
    " responds in the style of Dr Seuss.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem\\\n",
    " about a happy carrot\"\"\"},\n",
    "]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da77afc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnaby the carrot lived a cheerful life, his bright orange body nestled contentedly in the warm soil, dreaming of the day he'd be chosen for a delicious stew.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [\n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},\n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy carrot'},\n",
    "]\n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "504169b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configure API key\n",
    "genai.configure(api_key=\"API_KEY\")\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "\n",
    "def get_completion_and_token_count(messages,\n",
    "                                   temperature=0,\n",
    "                                   max_tokens=500):\n",
    "\n",
    "    # Convert OpenAI-style messages â†’ Gemini format\n",
    "    gemini_messages = []\n",
    "    for msg in messages:\n",
    "        gemini_messages.append({\n",
    "            \"role\": \"user\" if msg[\"role\"] == \"user\" else \"model\",\n",
    "            \"parts\": [{\"text\": msg[\"content\"]}]\n",
    "        })\n",
    "\n",
    "    # Gemini API call\n",
    "    response = model.generate_content(\n",
    "        gemini_messages,\n",
    "        generation_config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_tokens\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Extract text response\n",
    "    content = response.text\n",
    "\n",
    "    # Extract token usage (Gemini format)\n",
    "    usage = response.usage_metadata\n",
    "\n",
    "    token_dict = {\n",
    "        \"prompt_tokens\": usage.prompt_token_count,\n",
    "        \"completion_tokens\": usage.candidates_token_count,\n",
    "        \"total_tokens\": usage.total_token_count\n",
    "    }\n",
    "\n",
    "    return content, token_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1b2f02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a garden, green and grand,\n",
      "Grew a carrot, happy and tanned!\n",
      "With a wiggle and a grin,\n",
      "He'd say, \"Oh, what fun to begin!\"\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "{'role':'system',\n",
    " 'content':\"\"\"You are an assistant who responds\\\n",
    " in the style of Dr Seuss.\"\"\"},\n",
    "{'role':'user',\n",
    " 'content':\"\"\"write me a very short poem \\\n",
    " about a happy carrot\"\"\"},\n",
    "]\n",
    "response, token_dict = get_completion_and_token_count(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4447ad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 27, 'completion_tokens': 40, 'total_tokens': 67}\n"
     ]
    }
   ],
   "source": [
    "print(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5a566",
   "metadata": {},
   "source": [
    "###Install the Google Generative AI Python Library\n",
    "!pip install google-generativeai\n",
    "\n",
    "2. Get Your API Key\n",
    "\n",
    "Visit the official Gemini API key page:\n",
    "\n",
    "ðŸ”— https://aistudio.google.com/app/apikey\n",
    "\n",
    "Copy your API key.\n",
    "\n",
    "3. Set Your API Key (Option 1: Environment Variable)\n",
    "!export GOOGLE_API_KEY=\"your-api-key-here\"\n",
    "\n",
    "4. Set API Key in Python (Option 2: In Code)\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"your-api-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2efc02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
